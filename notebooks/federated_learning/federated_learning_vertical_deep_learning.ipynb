{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated learning: deep learning for vertically partitioned data \n",
    "\n",
    "In this notebook, we provide a simple example of how to perform a **vertical** federated learning experiment with the help of the Sherpa.ai Federated Learning framework. \n",
    "As opposed to the horizontal federated learning paradigm, in a vertical federated learning setting (see e.g. [Federated Machine Learning: Concept and Applications](https://arxiv.org/abs/1902.04885)) the different nodes possess the same samples, but different features. \n",
    "A practical example being that of a local on-line shop and an insurance company: both entities might have matching customers (samples), but the information (features) each entity possesses about the customers is of different nature. \n",
    "We are going to use a synthetic dataset and a neural network model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data\n",
    "We use `sklearn` module for generating synthetic databases. \n",
    "Moreover, in order to simulate a vertically partitioned training data, we randomly split the features of the created dataset among the clients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from shfl.private.reproducibility import Reproducibility\n",
    "\n",
    "# Comment to turn off reproducibility:\n",
    "Reproducibility(567)\n",
    "\n",
    "# Create dataset\n",
    "n_features = 20\n",
    "n_classes = 2\n",
    "n_samples = 15000\n",
    "\n",
    "data, labels = make_classification(\n",
    "    n_samples=n_samples, n_features=n_features, \n",
    "    n_redundant=0, n_repeated=0, n_classes=n_classes, \n",
    "    n_clusters_per_class=1, flip_y=0.1, class_sep=0.4, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vertical split of the dataset.** In the vertical FL setting, the database is split along the columns (i.e., vertically) among the nodes. \n",
    "We can use a method provided by the Sherpa FL Framework to randomly split a dataset vertically into the desired number of parts: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shfl.data_base.data_base import vertical_split\n",
    "\n",
    "# Create a vertically split dataset: split the features among clients\n",
    "M = 2  # number of clients\n",
    "train_data, train_labels, test_data, test_labels = \\\n",
    "    vertical_split(data=data, labels=labels)\n",
    "\n",
    "for item in train_data:\n",
    "    print(\"Client train data shape: \" + str(item.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wrap into FederatedData.** At this point, we assign the data to a federated network of clients. \n",
    "Since the clients actually don't possess the labels (only the server does), we assign the client's labels to `None`. \n",
    "And since we already performed the split of data for each client, we just use `PlainDataDistribution` to create the wrapper for the federated data (this class converts the data \"as is\" into a `FederatedData` object):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make federated data: \n",
    "from shfl.data_base.data_base import LabeledDatabase\n",
    "from shfl.data_distribution.data_distribution_plain import PlainDataDistribution\n",
    "\n",
    "clients_labels = [None] * len(train_data) \n",
    "data_base = LabeledDatabase(data=train_data, \n",
    "                            labels=clients_labels, \n",
    "                            train_percentage=1., \n",
    "                            shuffle=False) \n",
    "data_base.load_data()\n",
    "federated_data, _, _ = PlainDataDistribution(database=data_base).get_federated_data()\n",
    "print(federated_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Federated transformation.** In case we need to perform a data transformation on all the clients of the federated data, this is easily achieved by the framework's method: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Federated data operation: need to transpose\n",
    "from shfl.private import FederatedTransformation\n",
    "\n",
    "class Transpose(FederatedTransformation):\n",
    "    \n",
    "    def apply(self, labeled_data):\n",
    "        if labeled_data.data is not None:\n",
    "            labeled_data.data = labeled_data.data.T\n",
    "        \n",
    "federated_data.apply_data_transformation(Transpose());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to visually check everything went fine with the data assignment, we can configure data access to node (otherwise protected by default):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check federated data:\n",
    "from shfl.private.data import UnprotectedAccess\n",
    "\n",
    "federated_data.configure_data_access(UnprotectedAccess());\n",
    "federated_data[0].query()\n",
    "#federated_data[0].query().data\n",
    "#federated_data[0].query().label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model:\n",
    "\n",
    "**Horizontal Vs Vertical Federated Learning.** Both in the  Federated Government is interpreted as follows: \n",
    " - The Federated Government is intended as a *Coordinator*: it defines and schedules the federated computations, but does not have any other function (no data, no model). It is what a user can customize for the specific case problem.\n",
    " - The Federated Data is composed by nodes that can have multiple functions: train, store data, aggregate, make auxiliary computations, predictions etc.\n",
    " - In particular, the Server is itself a *node* that can interact with the Federated Data: it might aggregate, but might also contain data and train on them\n",
    "\n",
    "In Horizontal FL (see e.g. the [basic concepts notebook](./federated_learning_basic_concepts.ipynb)), all nodes have typically the same model, and the server node has also the aggregation function in its model as an attribute but do not train and does not possess any data.\n",
    "Instead in a Vertical FL architecture, the client nodes might have a *different model* with respect each other and with respect the server node.\n",
    "The latter in turn can aggregate, train and might possess its own data (i.e. the labels in this case).\n",
    "\n",
    "Note that the distinction between client and server is *only virtual* and not necessarily physical, since a single node might be both client and server, allowing multiple roles for the same physical node.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the server node.** We said that in the Vertical FL, each node, including the server, is allowed to possess a different model and different methods for interacting with the clients.\n",
    "We here define the server model with specific functions needed for the present Vertical FL architecture.\n",
    "The server is assigned a linear model, along with the data to train on (only labels, in this specific example): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shfl.federated_government.vertical_federated_deep_learning import FederatedGovernmentVertical\n",
    "from shfl.model.vertical_deep_learning_model import VerticalNeuralNetClient\n",
    "from shfl.model.vertical_deep_learning_model import VerticalLogLinearServer\n",
    "from shfl.private.federated_operation import VerticalServerDataNode\n",
    "from shfl.private.data import LabeledData\n",
    "\n",
    "\n",
    "# Create the server node: \n",
    "server_node = VerticalServerDataNode(\n",
    "    federated_data=federated_data, \n",
    "    model=VerticalLogLinearServer(), \n",
    "    aggregator=None,\n",
    "    data=LabeledData(data=None, label=train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define specific data access needed for the Vertical FL round.** The specific Vertical FL architecture requires the computation of the Loss and the exchange of convergence parameters. \n",
    "Namely, the clients send the computed embeddings to the server, and the server sends the computed gradients to update the clients. \n",
    "Therefore, we define ad-hoc access definitions for these methods, and we assign them to server and clients: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shfl.private.data import DataAccessDefinition\n",
    "\n",
    "class ComputeLoss(DataAccessDefinition):\n",
    "    def apply(self, data, **kwargs): \n",
    "        embeddings = kwargs.get(\"embeddings\")\n",
    "        server_model = kwargs.get(\"server_model\")\n",
    "        loss = server_model.compute_loss(\n",
    "                embeddings, data.label)\n",
    "        \n",
    "        print(\"Collaborative model train loss: \" + str(loss))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "class QueryMetaParameters(DataAccessDefinition):\n",
    "    def apply(self, model, **kwargs):\n",
    "        \"\"\"Returns embeddings (or their gradients) as computed by the local model.\"\"\"\n",
    "        return model.get_meta_params(**kwargs)\n",
    "\n",
    "    \n",
    "# Configure data access to nodes and server\n",
    "federated_data.configure_model_access(QueryMetaParameters())\n",
    "server_node.configure_model_access(QueryMetaParameters())\n",
    "server_node.configure_data_access(ComputeLoss())   \n",
    "\n",
    "print(federated_data[1]._model_access_policy)\n",
    "print(server_node._model_access_policy)\n",
    "print(server_node._private_data_access_policies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the federated learning experiment\n",
    "\n",
    "We are almost done: we only need to specify which specific model to use for each client node, and the server node. \n",
    "Namely, the clients will run a neural network model, but of course they will have different input size since they possess different number of features. \n",
    "We first don't use hidden layers for the clients model, resulting in a *linear* model (`layer_dims=None` parameter):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nodes = [VerticalNeuralNetClient(n_features=train_data[0].shape[1], layer_dims=None),\n",
    "               VerticalNeuralNetClient(n_features=train_data[1].shape[1], layer_dims=None)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create federated government and run training:\n",
    "\n",
    "federated_government = FederatedGovernmentVertical(model_nodes, \n",
    "                                                   federated_data, \n",
    "                                                   server_node=server_node)\n",
    "\n",
    "test_data_T = [client_data.T for client_data in test_data]\n",
    "federated_government.run_rounds(n=10001, \n",
    "                                test_data=test_data_T, \n",
    "                                test_label=test_labels, \n",
    "                                print_freq=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison to Centralized training.** As reference, we can compare the performance of the collaborative model to the centralized training:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(y_test, y_prediction, save_path=None):\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prediction)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.rcParams.update({'font.size': 15})\n",
    "    plt.figure(figsize=(8, 7))\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    if save_path is not None: \n",
    "        plt.savefig(save_path, bbox_inches = \"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction = federated_government._server.evaluate_collaborative_model(\n",
    "    test_data=test_data_T, test_label=test_labels)\n",
    "plot_roc(test_labels, y_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear model Benchmark on centralized data using sk-learn:\n",
    "#clf_linear = MLPClassifier(hidden_layer_sizes=(1,), max_iter=10000, shuffle=False, random_state=321).fit(x_train, y_train)\n",
    "\n",
    "centralized_train_data = np.concatenate(train_data, axis=1)\n",
    "centralized_test_data = np.concatenate(test_data, axis=1)\n",
    "\n",
    "clf_linear = LogisticRegression(random_state=123).fit(centralized_train_data, train_labels)\n",
    "\n",
    "y_prediction = clf_linear.predict_proba(centralized_test_data)[:, 1]\n",
    "plot_roc(test_labels, y_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Non-linear model:**\n",
    "\n",
    "We now add a hidden layer in the clients' neural network model, resulting in a *non-linear* model (parameter `layer_dims=[3]`): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nodes = [VerticalNeuralNetClient(n_features=train_data[0].shape[1], layer_dims=[3]),\n",
    "               VerticalNeuralNetClient(n_features=train_data[1].shape[1], layer_dims=[3])]\n",
    "\n",
    "federated_government = FederatedGovernmentVertical(model_nodes, \n",
    "                                                   federated_data, \n",
    "                                                   server_node=server_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "federated_government.run_rounds(n=100001, \n",
    "                                test_data=test_data_T, \n",
    "                                test_label=test_labels, \n",
    "                                print_freq=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can compare the performance to the analogous centralized model using one hidden layer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction = federated_government._server.evaluate_collaborative_model(\n",
    "    test_data=test_data_T, test_label=test_labels)\n",
    "plot_roc(test_labels, y_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-linear benchmark\n",
    "clf_non_linear = MLPClassifier(hidden_layer_sizes=(3,), max_iter=10000, \n",
    "                               shuffle=False, random_state=3221)\n",
    "clf_non_linear.fit(centralized_train_data, train_labels)\n",
    "\n",
    "y_prediction = clf_non_linear.predict_proba(centralized_test_data)[:, 1]\n",
    "plot_roc(test_labels, y_prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SherpaFL_py37",
   "language": "python",
   "name": "sherpafl_py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
