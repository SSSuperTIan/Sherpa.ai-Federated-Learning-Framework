{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated learning: using a TensorFlow model\n",
    "\n",
    "In this notebook, we provide a simple example of how to perform an experiment in a federated environment, with the help of Sherpa.ai Federated Learning framework. \n",
    "## The data\n",
    "We are going to use a popular dataset: the framework provides some functions for loading the [Emnist](https://www.nist.gov/itl/products-and-services/emnist-dataset) digits dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shfl\n",
    "\n",
    "database = shfl.data_base.Emnist()\n",
    "train_data, train_labels, test_data, test_labels = database.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect some properties of the loaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_data))\n",
    "print(len(test_data))\n",
    "print(type(train_data[0]))\n",
    "train_data[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, as we have seen, our dataset is composed of a set of matrices that are 28 by 28. Before starting with the federated scenario, we can take a look at a sample in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to simulate a federated learning scenario with a set of client nodes containing private data, and a central server that will be responsible for coordinating the different clients. But, first of all, we have to simulate the data contained in every client. In order to do that, we are going to use the previously loaded dataset. The assumption in this example is that the data is distributed as a set of independent and identically distributed random variables, with every node having approximately the same amount of data. There are a set of different possibilities for distributing the data. The distribution of the data is one of the factors that can have the most impact on a federated algorithm. Therefore, the framework has some of the most common distributions implemented, which allows you to easily experiment with different situations. In [Sampling Methods](./federated_learning_sampling.ipynb), you can dig into the options that the framework provides, at the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iid_distribution = shfl.data_distribution.IidDataDistribution(database)\n",
    "federated_data, test_data, test_label = iid_distribution.get_federated_data(num_nodes=20, percent=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! We have created federated data from the Emnist dataset using 20 nodes and 10 percent of the available data. This data is distributed to a set of data nodes in the form of private data. Let's learn a little more about the federated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(federated_data))\n",
    "print(federated_data.num_nodes())\n",
    "federated_data[0].private_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, private data in a node is not directly accessible but the framework provides mechanisms to use this data in a machine learning model. \n",
    "## The model\n",
    "A federated learning algorithm is defined by a machine learning model, locally deployed in each node, that learns from the respective node's private data and an aggregating mechanism to aggregate the different model parameters uploaded by the client nodes to a central node. In this example we will build a Tensorflow learning model. The framework provides classes to allow using Tensorflow and Keras (see [Basic Concepts](./federated_learning_basic_concepts.ipynb)) models into a federated learning scenario, your job is only to create a function acting as model builder. Moreover, the framework provides classes to allow using pretrained Tensorflow and Keras models (see [Pretrained Model](./federated_learning_basic_concepts_pretrained_model.ipynb)).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#If you want execute in GPU, you must uncomment this two lines.\n",
    "# physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "class CustomDense(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Implementation of Linear layer\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    units : int\n",
    "        number of units for the output\n",
    "    w : matrix\n",
    "        Weights from the layer\n",
    "    b : array\n",
    "        Bias from the layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, units=32, **kwargs):\n",
    "        super(CustomDense, self).__init__(**kwargs)\n",
    "        self._units = units\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = {'units': self._units}\n",
    "        base_config = super(CustomDense, self).get_config()\n",
    "\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        Method for build the params\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_shape: list\n",
    "            size of inputs\n",
    "        \"\"\"\n",
    "        self._w = self.add_weight(shape=(input_shape[-1], self._units),\n",
    "                                  initializer='random_normal',\n",
    "                                  trainable=True)\n",
    "\n",
    "        self._b = self.add_weight(shape=(self._units,),\n",
    "                                  initializer='random_normal',\n",
    "                                  trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Apply linear layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs: matrix\n",
    "            Input data\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        result : matrix\n",
    "            the result of linear transformation of the data\n",
    "        \"\"\"\n",
    "        return tf.nn.bias_add(tf.matmul(inputs, self._w), self._b)\n",
    "\n",
    "\n",
    "def model_builder():\n",
    "    inputs = tf.keras.Input(shape=(28, 28, 1))\n",
    "    x = tf.keras.layers.Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu', strides=1)(inputs)\n",
    "    x = tf.keras.layers.MaxPooling2D(pool_size=2, strides=2, padding='valid')(x)\n",
    "    x = tf.keras.layers.Dropout(0.4)(x)\n",
    "    x = tf.keras.layers.Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu', strides=1)(x)\n",
    "    x = tf.keras.layers.MaxPooling2D(pool_size=2, strides=2, padding='valid')(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = CustomDense(128)(x)\n",
    "    x = tf.nn.relu(x)\n",
    "    x = tf.keras.layers.Dropout(0.1)(x)\n",
    "    x = CustomDense(64)(x)\n",
    "    x = tf.nn.relu(x)\n",
    "    x = CustomDense(10)(x)\n",
    "    outputs = tf.nn.softmax(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    return shfl.model.DeepLearningModel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the only piece missing is the aggregation operator. Nevertheless, the framework provides some aggregation operators that we can use. In the following piece of code, we define the federated aggregation mechanism. Moreover, we define the federated government based on the Tensorflow learning model, the federated data, and the aggregation mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregator = shfl.federated_aggregator.FedAvgAggregator()\n",
    "federated_government = shfl.federated_government.FederatedGovernment(model_builder, federated_data, aggregator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to see all the aggregation operators, you can check out the [Aggregation Operators](./federated_learning_basic_concepts_aggregation_operators.ipynb) notebook. Before running the algorithm, we want to apply a transformation to the data. A good practice is to define a federated operation that will ensure that the transformation is applied to the federated data in all the client nodes. We want to reshape the data, so we define the following FederatedTransformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Reshape(shfl.private.FederatedTransformation):\n",
    "    \n",
    "    def apply(self, labeled_data):\n",
    "        labeled_data.data = np.reshape(labeled_data.data, (labeled_data.data.shape[0], labeled_data.data.shape[1], labeled_data.data.shape[2],1))\n",
    "\n",
    "class CastFloat(shfl.private.FederatedTransformation):\n",
    "    \n",
    "    def apply(self, labeled_data):\n",
    "        labeled_data.data = labeled_data.data.astype(np.float32)\n",
    "        \n",
    "shfl.private.federated_operation.apply_federated_transformation(federated_data, Reshape())\n",
    "shfl.private.federated_operation.apply_federated_transformation(federated_data, CastFloat())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the federated learning experiment\n",
    "We are now ready to execute our federated learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.reshape(test_data, (test_data.shape[0], test_data.shape[1], test_data.shape[2],1))\n",
    "test_data = test_data.astype(np.float32)\n",
    "federated_government.run_rounds(3, test_data, test_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SherpaFL_py37",
   "language": "python",
   "name": "sherpafl_py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
